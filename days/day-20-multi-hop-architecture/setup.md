# Day 20: Setup - Multi-Hop Architecture

## Environment Setup

### 1. Create Databases

```python
# Create databases for each layer
spark.sql("CREATE DATABASE IF NOT EXISTS bronze")
spark.sql("CREATE DATABASE IF NOT EXISTS silver")
spark.sql("CREATE DATABASE IF NOT EXISTS gold")

# Set default database
spark.sql("USE bronze")

# Create checkpoint directories
checkpoint_base = "/tmp/checkpoints/day20"
dbutils.fs.mkdirs(checkpoint_base)
dbutils.fs.mkdirs(f"{checkpoint_base}/bronze")
dbutils.fs.mkdirs(f"{checkpoint_base}/silver")
dbutils.fs.mkdirs(f"{checkpoint_base}/gold")

print("âœ“ Databases and checkpoints created")
```

### 2. Sample Data Creation

#### Dataset 1: Raw Event Data (Bronze Source)

```python
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime, timedelta
import random
import json

# Generate raw event data (simulating source system)
raw_events = []
base_time = datetime(2024, 1, 1, 10, 0, 0)

for i in range(1, 101):
    event_time = base_time + timedelta(minutes=random.randint(0, 1440))
    raw_events.append({
        "event_id": i,
        "user_id": random.randint(1, 20),
        "event_type": random.choice(["login", "view", "click", "purchase"]),
        "amount": round(random.uniform(10, 500), 2) if random.random() > 0.3 else None,
        "timestamp": event_time.strftime("%Y-%m-%d %H:%M:%S"),
        "device": random.choice(["mobile", "desktop", "tablet"]),
        "country": random.choice(["US", "UK", "CA", "AU"])
    })

# Add some duplicates and bad data
raw_events.append(raw_events[0])  # Duplicate
raw_events.append({"event_id": None, "user_id": 1, "event_type": "bad"})  # Bad data

# Create Bronze table (raw data)
bronze_schema = StructType([
    StructField("event_id", IntegerType(), True),
    StructField("user_id", IntegerType(), True),
    StructField("event_type", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("timestamp", StringType(), True),
    StructField("device", StringType(), True),
    StructField("country", StringType(), True)
])

bronze_df = spark.createDataFrame([tuple(e.values()) for e in raw_events], 
                                  schema=["event_id", "user_id", "event_type", "amount", 
                                         "timestamp", "device", "country"])

# Add ingestion metadata
bronze_with_metadata = (
    bronze_df
    .withColumn("ingestion_time", current_timestamp())
    .withColumn("source_file", lit("raw_events_batch_1.json"))
)

# Write to Bronze
(bronze_with_metadata
    .write
    .format("delta")
    .mode("overwrite")
    .saveAsTable("bronze.events")
)

print(f"âœ“ Created bronze.events with {bronze_with_metadata.count()} records (includes duplicates and bad data)")
```

#### Dataset 2: Additional Raw Data Batches

```python
# Batch 2 - More recent data
raw_events_batch2 = []
base_time2 = datetime(2024, 1, 2, 10, 0, 0)

for i in range(101, 151):
    event_time = base_time2 + timedelta(minutes=random.randint(0, 720))
    raw_events_batch2.append({
        "event_id": i,
        "user_id": random.randint(1, 20),
        "event_type": random.choice(["login", "view", "click", "purchase"]),
        "amount": round(random.uniform(10, 500), 2) if random.random() > 0.3 else None,
        "timestamp": event_time.strftime("%Y-%m-%d %H:%M:%S"),
        "device": random.choice(["mobile", "desktop", "tablet"]),
        "country": random.choice(["US", "UK", "CA", "AU"])
    })

bronze_df_batch2 = spark.createDataFrame([tuple(e.values()) for e in raw_events_batch2],
                                         schema=["event_id", "user_id", "event_type", "amount",
                                                "timestamp", "device", "country"])

bronze_batch2_with_metadata = (
    bronze_df_batch2
    .withColumn("ingestion_time", current_timestamp())
    .withColumn("source_file", lit("raw_events_batch_2.json"))
)

# Save for incremental processing exercises
(bronze_batch2_with_metadata
    .write
    .format("delta")
    .mode("overwrite")
    .saveAsTable("bronze.events_batch2")
)

print(f"âœ“ Created bronze.events_batch2 with {bronze_batch2_with_metadata.count()} records")
```

#### Dataset 3: Empty Silver Table

```python
# Create empty Silver table with proper schema
silver_schema = StructType([
    StructField("event_id", IntegerType(), False),
    StructField("user_id", IntegerType(), False),
    StructField("event_type", StringType(), False),
    StructField("amount", DoubleType(), True),
    StructField("timestamp", TimestampType(), False),
    StructField("device", StringType(), True),
    StructField("country", StringType(), True),
    StructField("ingestion_time", TimestampType(), False),
    StructField("processed_time", TimestampType(), False)
])

empty_silver_df = spark.createDataFrame([], silver_schema)

(empty_silver_df
    .write
    .format("delta")
    .mode("overwrite")
    .saveAsTable("silver.events")
)

print("âœ“ Created empty silver.events table")
```

#### Dataset 4: Empty Gold Tables

```python
# Gold - Daily Metrics
gold_daily_schema = StructType([
    StructField("date", DateType(), False),
    StructField("event_type", StringType(), False),
    StructField("event_count", LongType(), False),
    StructField("total_amount", DoubleType(), True),
    StructField("avg_amount", DoubleType(), True),
    StructField("unique_users", LongType(), False)
])

empty_gold_daily = spark.createDataFrame([], gold_daily_schema)

(empty_gold_daily
    .write
    .format("delta")
    .mode("overwrite")
    .saveAsTable("gold.daily_metrics")
)

print("âœ“ Created empty gold.daily_metrics table")

# Gold - Hourly Metrics
gold_hourly_schema = StructType([
    StructField("hour_start", TimestampType(), False),
    StructField("hour_end", TimestampType(), False),
    StructField("event_type", StringType(), False),
    StructField("event_count", LongType(), False),
    StructField("total_amount", DoubleType(), True),
    StructField("unique_users", LongType(), False)
])

empty_gold_hourly = spark.createDataFrame([], gold_hourly_schema)

(empty_gold_hourly
    .write
    .format("delta")
    .mode("overwrite")
    .saveAsTable("gold.hourly_metrics")
)

print("âœ“ Created empty gold.hourly_metrics table")

# Gold - User Summary
gold_user_schema = StructType([
    StructField("user_id", IntegerType(), False),
    StructField("total_events", LongType(), False),
    StructField("total_amount", DoubleType(), True),
    StructField("first_event", TimestampType(), True),
    StructField("last_event", TimestampType(), True),
    StructField("favorite_device", StringType(), True)
])

empty_gold_user = spark.createDataFrame([], gold_user_schema)

(empty_gold_user
    .write
    .format("delta")
    .mode("overwrite")
    .saveAsTable("gold.user_summary")
)

print("âœ“ Created empty gold.user_summary table")
```

#### Dataset 5: Product Data (for enrichment)

```python
# Dimension table for enrichment
products_data = [
    (1, "Premium Subscription", "subscription", 99.99),
    (2, "Basic Subscription", "subscription", 49.99),
    (3, "E-book", "digital", 19.99),
    (4, "Video Course", "digital", 79.99),
    (5, "Consulting Hour", "service", 150.00)
]

products_schema = StructType([
    StructField("product_id", IntegerType(), False),
    StructField("product_name", StringType(), False),
    StructField("category", StringType(), False),
    StructField("price", DoubleType(), False)
])

products_df = spark.createDataFrame(products_data, products_schema)

(products_df
    .write
    .format("delta")
    .mode("overwrite")
    .saveAsTable("silver.products")
)

print("âœ“ Created silver.products dimension table")
```

### 3. Helper Functions

```python
def show_layer_summary():
    """Display record counts for all layers"""
    print("\n" + "="*70)
    print("MEDALLION ARCHITECTURE - LAYER SUMMARY")
    print("="*70)
    
    # Bronze
    bronze_count = spark.table("bronze.events").count()
    print(f"\nğŸ¥‰ BRONZE LAYER (Raw Data)")
    print(f"   bronze.events: {bronze_count} records")
    
    # Silver
    silver_count = spark.table("silver.events").count()
    print(f"\nğŸ¥ˆ SILVER LAYER (Cleaned Data)")
    print(f"   silver.events: {silver_count} records")
    print(f"   silver.products: {spark.table('silver.products').count()} records")
    
    # Gold
    print(f"\nğŸ¥‡ GOLD LAYER (Business Aggregates)")
    print(f"   gold.daily_metrics: {spark.table('gold.daily_metrics').count()} records")
    print(f"   gold.hourly_metrics: {spark.table('gold.hourly_metrics').count()} records")
    print(f"   gold.user_summary: {spark.table('gold.user_summary').count()} records")
    
    print("="*70 + "\n")

def show_data_quality_issues():
    """Show data quality issues in Bronze"""
    print("\n" + "="*70)
    print("BRONZE LAYER - DATA QUALITY ISSUES")
    print("="*70)
    
    bronze_df = spark.table("bronze.events")
    
    total = bronze_df.count()
    null_event_ids = bronze_df.filter(col("event_id").isNull()).count()
    null_timestamps = bronze_df.filter(col("timestamp").isNull()).count()
    duplicates = total - bronze_df.select("event_id").distinct().count()
    
    print(f"Total records: {total}")
    print(f"Null event_ids: {null_event_ids}")
    print(f"Null timestamps: {null_timestamps}")
    print(f"Duplicate event_ids: {duplicates}")
    print("="*70 + "\n")

def compare_layers(event_id):
    """Compare how a record looks across layers"""
    print(f"\n{'='*70}")
    print(f"RECORD COMPARISON ACROSS LAYERS - Event ID: {event_id}")
    print('='*70)
    
    # Bronze
    print("\nğŸ¥‰ BRONZE (Raw):")
    spark.table("bronze.events").filter(col("event_id") == event_id).show(truncate=False)
    
    # Silver
    print("\nğŸ¥ˆ SILVER (Cleaned):")
    silver_df = spark.table("silver.events").filter(col("event_id") == event_id)
    if silver_df.count() > 0:
        silver_df.show(truncate=False)
    else:
        print("   (Not yet processed to Silver)")
    
    print('='*70 + "\n")

def show_pipeline_flow():
    """Visualize the pipeline flow"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    MEDALLION ARCHITECTURE FLOW                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                    â•‘
â•‘  ğŸ“ Raw Files                                                      â•‘
â•‘       â†“                                                            â•‘
â•‘  ğŸ¥‰ BRONZE LAYER (bronze.events)                                   â•‘
â•‘       â€¢ Raw data as-is                                             â•‘
â•‘       â€¢ Minimal transformation                                     â•‘
â•‘       â€¢ Includes bad data & duplicates                             â•‘
â•‘       â€¢ Add ingestion metadata                                     â•‘
â•‘       â†“                                                            â•‘
â•‘  ğŸ¥ˆ SILVER LAYER (silver.events)                                   â•‘
â•‘       â€¢ Data quality checks                                        â•‘
â•‘       â€¢ Deduplication                                              â•‘
â•‘       â€¢ Type conversions                                           â•‘
â•‘       â€¢ Standardization                                            â•‘
â•‘       â€¢ Enrichment                                                 â•‘
â•‘       â†“                                                            â•‘
â•‘  ğŸ¥‡ GOLD LAYER (gold.*)                                            â•‘
â•‘       â€¢ Business aggregates                                        â•‘
â•‘       â€¢ Daily/hourly metrics                                       â•‘
â•‘       â€¢ User summaries                                             â•‘
â•‘       â€¢ Ready for BI tools                                         â•‘
â•‘                                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

# Display initial state
show_layer_summary()
show_data_quality_issues()
show_pipeline_flow()
```

### 4. Quick Reference

```python
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         DAY 20: MULTI-HOP ARCHITECTURE - QUICK REFERENCE           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Databases Created:                                                 â•‘
â•‘   â€¢ bronze - Raw data layer                                        â•‘
â•‘   â€¢ silver - Cleaned data layer                                    â•‘
â•‘   â€¢ gold - Business aggregates layer                               â•‘
â•‘                                                                    â•‘
â•‘ Tables Created:                                                    â•‘
â•‘   ğŸ¥‰ Bronze:                                                        â•‘
â•‘      â€¢ bronze.events (102 records with issues)                     â•‘
â•‘      â€¢ bronze.events_batch2 (50 records for incremental)           â•‘
â•‘                                                                    â•‘
â•‘   ğŸ¥ˆ Silver:                                                        â•‘
â•‘      â€¢ silver.events (empty - to be populated)                     â•‘
â•‘      â€¢ silver.products (5 products for enrichment)                 â•‘
â•‘                                                                    â•‘
â•‘   ğŸ¥‡ Gold:                                                          â•‘
â•‘      â€¢ gold.daily_metrics (empty)                                  â•‘
â•‘      â€¢ gold.hourly_metrics (empty)                                 â•‘
â•‘      â€¢ gold.user_summary (empty)                                   â•‘
â•‘                                                                    â•‘
â•‘ Helper Functions:                                                  â•‘
â•‘   â€¢ show_layer_summary()                                           â•‘
â•‘   â€¢ show_data_quality_issues()                                     â•‘
â•‘   â€¢ compare_layers(event_id)                                       â•‘
â•‘   â€¢ show_pipeline_flow()                                           â•‘
â•‘                                                                    â•‘
â•‘ Key Concepts:                                                      â•‘
â•‘   â€¢ Bronze = Raw data (as-is)                                      â•‘
â•‘   â€¢ Silver = Cleaned & validated                                   â•‘
â•‘   â€¢ Gold = Business aggregates                                     â•‘
â•‘   â€¢ Incremental processing at each layer                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
```

## Setup Complete! âœ…

You now have:
- âœ… Three databases (bronze, silver, gold)
- âœ… Bronze layer with raw data (includes quality issues)
- âœ… Empty Silver and Gold tables ready for processing
- âœ… Dimension table for enrichment
- âœ… Helper functions for analysis
- âœ… Ready for multi-hop pipeline exercises!

**Next Step**: Open `exercise.py` and start building your Medallion Architecture!
